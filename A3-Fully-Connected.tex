\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
% hyper links
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
% Formatting quotes properly
\usepackage[english]{babel}
\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}


\begin{document}
\noindent Author: Benjamin Smidt

\noindent Created: September 19, 2022

\noindent Last Updated: September 19, 2022
\begin{center}
\section*{Assignment 2: Full Connected Networks}
\end{center}

\paragraph{} \emph{Note to reader.} 

This is my work for assignment three of Michigan's course
\href{https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/}
{EECS 498: Deep Learning for Computer Vision}. The majority of explanations and understanding are 
derived from \href{https://www.youtube.com/watch?v=dJYGatp4SvA&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&index=1}
{Justin Johnson's Lectures} and \href{http://cs231n.stanford.edu/schedule.html}{Stanford's CS 231N Lecture Notes}.
This document is meant to be used as a reference, 
explanation, and resource for the assignment, not necessarily a comprehensive overview
of Neural Networks. If there's a typo or a correction needs to be made, feel free to 
email me at benjamin.smidt@utexas.edu so I can fix it. Thank you! I hope you find this 
document helpful.

\tableofcontents{}

\section{Functions}

\subsection{Linear.Forward}
The \emph{Forward} function, implemented in the \emph{Linear} class is simple. We aren't doing 
anything different from the matrix multiplication and bias computed in the two layer
neural network we built last assignment. See \href{https://github.com/bensmidt/EECS-498-DL-Computer-Vision/blob/main/A2/A2-Two-Layer-NN.pdf}
{A2: Two Layer Neural Net} for an in depth explanation of how this matrix multiplication was 
computed. 

\subsection{Linear.Backward}
The \emph{Backward} function, implemented in the \emph{Linear} class certainly isn't
as simply as the forward function. However, I explained how to compute the gradient of 
a matrix multiplication for $x$, $w$, and $b$ in great length in the last assignment 
\href{https://github.com/bensmidt/EECS-498-DL-Computer-Vision/blob/main/A2/A2-Two-Layer-NN.pdf}
{A2: Two Layer Neural Net}.

\subsection{ReLU.Forward}
Just look up the documentation for torch.clamp() if you're confused. It's just setting
every value less than zero equal to zero and leaving positive values alone. 

\subsection{ReLU.Backward}
This is getting repetitive. Again, see A2: Two Layer Neural Net for information on ReLU. 
I explain, in depth, both the forward and backward pass of ReLU including the gradient 
and how to compute it. 

\subsection{Linear-ReLU.forward and Linear-ReLU.backward}
These functions are just utilizing the functions we just created. Although, it's worth 
noting how beautiful the modularity of this code truly is. With no extra work we were 
able to easily define a common function. Obviously, I will not be explaining object oriented 
programming (OOP). That is literally a course in and of itself. However, I did just want to 
stop for a second and marvel at the beauty that OOP has produced here. 

\subsection{Softmax and SVM}
Since we spent a LOT of time deriving, explaining, and programming the SVM and Softmax
algorithms in assigment 2, they were nice enough to gift us this implementation. To be clear, 
the following code is NOT my own. It is freely available from the 
\href{https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/assignment3.html} 
{EECS 498 Course Website} in assignment 3. If you'd like to see my implementation (which is almost
certainly slightly worse), you can check out 
\href{https://github.com/bensmidt/EECS-498-DL-Computer-Vision/blob/main/A2/A2-Softmax.pdf}{A2: Softmax}
or \href{https://github.com/bensmidt/EECS-498-DL-Computer-Vision/blob/main/A2/A2-SVM.pdf}{A2: Multiclass SVM}.

\paragraph{}
\subsubsection{SVM-Loss-Gradient}
\begin{verbatim}
def svm_loss(x, y):
    """
    Computes the loss and gradient using for multiclass SVM classification.
    Inputs:
    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth
      class for the ith input.
    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and
      0 <= y[i] < C
    Returns a tuple of:
    - loss: Scalar giving the loss
    - dx: Gradient of the loss with respect to x
    """
    N = x.shape[0]
    correct_class_scores = x[torch.arange(N), y]
    margins = (x - correct_class_scores[:, None] + 1.0).clamp(min=0)
    margins[torch.arange(N), y] = 0.
    loss = margins.sum() / N
    num_pos = (margins > 0).sum(dim=1)
    dx = torch.zeros_like(x)
    dx[margins > 0] = 1.
    dx[torch.arange(N), y] -= num_pos.to(dx.dtype)
    dx /= N
    return loss, dx
\end{verbatim}

Let's quickly review how this code computes SVM loss. 

\begin{equation}
    L_{i} = \sum_{j \neq y_{i}}^C max(0, x_{j} - x_{y_{i}} + \Delta)
\end{equation}

First, we get the correct class scores. Then we compute the margins by 
subtracting each value from the correct score in it's example, 
adding $\Delta$ (=1), and finally taking the max of that value and zero. 
Remember, this has the interpretation of SVM wanting the difference between 
the correct class score and every other score to be $\geq \Delta$. Lastly, 
we set all the correct class scores equal to zero and average the losses 
over each example. 

The derivatives of SVM with respect to $x_j$ and $x_{y_i}$ are as follows. 
\begin{equation}
    \nabla_{x_{y_i}} L_i = -\sum_{j \neq y_i}^C \mathbbm{1}
        (x_j - x_{y_i} + \Delta > 0)
\end{equation}

\begin{equation}
    \nabla_{x_j} L_i = \sum_{j \neq y_i}^C \mathbbm{1}
        (x_j - x_{y_i} + \Delta > 0)
\end{equation}

This is somewhat remiscient of the ReLU function since we have to deal with the 
$max()$ function. Regarding code though, for each example we simply sum the number of terms 
with a margin greater than zero. Then we set the gradient value of the correct score at a given example N
equal to this value, giving us the gradient with respect to $x_{y_i}$. For the gradient
with respect to $x_j$, we simply set all the incorrect scores index values to 1 if their
margin exceeds 0 and (leave it at) zero otherwise. Finally we divide all the values by 
N since we average the values over N examples in our loss function (if we summed them in 
our loss function then we wouldn't divide by N). 

\paragraph{}
\subsubsection{Softmax-Loss-Gradient}
\begin{verbatim}
def softmax_loss(x, y):
    """
    Computes the loss and gradient for softmax classification.
    Inputs:
    - x: Input data, of shape (N, C) where x[i, j] is the score for
      the jth class for the ith input.
    - y: Vector of labels, of shape (N,) where y[i] is the label
      for x[i] and 0 <= y[i] < C
    Returns a tuple of:
    - loss: Scalar giving the loss
    - dx: Gradient of the loss with respect to x
    """
    shifted_logits = x - x.max(dim=1, keepdim=True).values
    Z = shifted_logits.exp().sum(dim=1, keepdim=True)
    log_probs = shifted_logits - Z.log()
    probs = log_probs.exp()
    N = x.shape[0]
    loss = (-1.0 / N) * log_probs[torch.arange(N), y].sum()
    dx = probs.clone()
    dx[torch.arange(N), y] -= 1
    dx /= N
    return loss, dx
\end{verbatim}

For softmax, our loss is: 

\begin{equation}
    L_{i} = -log(\frac{e^{x_{y_{i}}}}{\sum_{j=1}^C e^{x_j}}) 
    = -x_{y_{i}} + log(\sum_{j=1}^C e^{x_{j}})
\end{equation}

In the code, we first normalize the everything to improve numerical stability. 
We can shift everything by any constant K and the loss won't change. 

\begin{equation}
    \frac{K e^{x_{y_{i}}}}{K \sum_{j=1}^C e^{x_j}} = 
    \frac{e^{x_{y_{i}} + logK}}{\sum_{j=1}^C e^{x_j + log K}} 
\end{equation}

For simplicity, we often choose $log K$ to be the negative of the max value in each example 
$e^x$. This prevents any single $e^{x_j}$ value from becoming too large
and subsequently producing a NaN or infinity. 

Anyways, we compute the loss function by adding the correct class score values
to the log of the sum of the scores in each example. Then we multiply by negative 1
and average over N examples. Personally, I like to distribute the negative 1 instead 
of multiplying at the end as seen in my loss function equation above (Eq. 4), but it's all
the same at the end of the day. 

Lasly, here are the derivatives. 

\begin{equation}
    \frac{\partial L_{i}}{\partial x_{y_i}} = \frac{e^{x_{y_i}}}
    {\sum_{j=1}^C e^{x_{j}}} - 1
\end{equation}

\begin{equation}
    \frac{\partial L_{i}}{\partial x_j} = \frac{e^{x_{j}}}
    {\sum_{j=1}^C e^{x_j}}
\end{equation}

To compute the gradient we simply reuse the \emph{probs} variable which stores
all the values of both gradients we need. We simply alter the correct score
indices by subtracting 1 and finish by dividing by N since, again, we average the loss 
over N examples. 

\subsection{TwoLayerNet Class}

\subsubsection{Initialization}
We use a two layer neural net class to to easily define our neural net. We begin by initilalizing
a dictionary, \emph{self.params}, and a float \emph{self.reg}. \emph{self.reg} is, of course, 
our regularization constant $\lambda$ and \emph{self.params} stores our changeable parameters 
in our network. Since we're creating a two later network with an architecture of fully connected
layers Linear - ReLU - Linear - Softmax (same as A2), we'll need four parameters. $W1$ and $b1$ our 
for our first Linear operation and $W2$ and $b2$ are for our second linear operation. 

We initialize each of the weight matrices $W1$ and $W2$ using a random normal distribution 
with mean 0 and standard deviation \emph{weight-scale}, a user input value into the TwoLayerNet Class
that defaults to $1*10^{-3}$

\subsubsection{Loss}
We can easily define our forward pass through the network using the classes we just created. 
In particular we call \emph{Linear-ReLU.forward} with $W1$ and $b1$ and pass the resulting 
hidden matrix to \emph{Linear.forward} with $W2$ and $b2$. Finally, we pass the outputted scores
matrix to \emph{softmax-loss}, the function defined and reviewed in the previous page, and add 
our regularization loss to the data loss computed by \emph{softmax-loss}. 

\subsubsection{Gradient}
Similar to our forward pass, we easily define our backward pass using the functions and classes 
we've created. The key here is really understanding what's being input and returned from our 
classes and what data from our forward pass (\emph{cache} in the code) needs to be passed to 
our function computing the backward pass. It's pretty simply, so I'll leave it to you to see how 
it's implemented in the code. Everything is nicely labeled so there shouldn't be any surprises. 
Just don't forget to add the gradient for your regularization loss! 

\subsection{FullyConnectedNet Class}

\subsubsection{Initialization}
When writing this I have NOT implemented different update rules (SGD + Momentum, RMSProp, Adam, Dropout). 
I'll explain those later in this document since the first implementation of FullyConnectedNet did 
not include implementing these various update rules. 

So, for initialization, I essentially just did the exact same process as the two layer network
except I generalized the hidden layers. I had to hard code the first and last layers since the first
layer is the only one that uses \emph{input-dim} (number of dimensions for each example in $X$) and 
the last layer is the only one that uses \emph{num-classes}. There is some python specific syntax
I used for storing an arbitrary number of parameters in \emph{self.params}. You should be able 
to easily look up Python documentation for what they do though. I'm just iteratively labelling 
$W$ and $b$ with a number to keep track of the parameters, nothing too exciting. 

\subsubsection{Loss}
Things can get a little hairy here but the idea is still the exact same as the two layer network
we just implemented. I iteratively pass the output of the \emph{Linear-ReLU.forward()} function
as the input of the next \emph{Linear-ReLU.forward()} function until I reach the very last layer. 
To compute the final \emph{scores} matrix we need to remember to use \emph{Linear} without \emph{ReLU}. 
We can then easily pass the scores to softmax which kindly returns our loss and \emph{d-scores} to 
begin backpropagating. 

Some easily forgetten things I need to mention. First, we need to store all the \emph{caches} or 
"weights" that we're computing throughout the network. We will need them for backpropagation. Second, 
it's very easy to forget to add regularization. This must be done iteratively as well and is easily 
computed with the for loop we're already using in our forward propagation. 

\subsubsection{Gradient}
Finally, we get to backprop. I've yet to mention this for previous assignments but I do want to 
point out the if-statement that catches whether  we're in "train" or "test" mode. Obviously, if 
we're in test mode we can't do backprop because we have no "correct answers" with which to 
compute our loss function. It's an important detail that can be easily missed, so yeah. 

Again, we iteratively backpropagate through our network in the same manner we've done for our 
two layer network. The last layer (first computation) must be done first since it's the only 
layer using \emph{Linear} while the rest can be computed in our for-loop using \emph{Linear-ReLU}. 
As a reminder, don't forget the regularization (I'm bad about that :( ). 

\subsection{Update Rules}
\subsubsection{SGD + Momentum}

Physics. Everything always comes back to physics. This idea is simply yet very powerful.
To improve stochastic gradient descent (SGD), we can add "momentum", a fancy word for saying: the
longer our gradient has been moving in the same direction along a given axis, the larger we 
want that step size to be along that axis. In the same vein, more momentum should make it more 
difficult to reverse the velocity along that axis. 

Why does this work? Well, it gives more weight
to the directions that are consistently updating in the same direction. By doing so, oscillating 
updates that you might find in vanila SGD are massively reduced. If every update the direction along
an axis is changing directions, then the momentum will push its update to be close to zero since 
the oscillating updates aren't getting us anywhere meaningful. Essentially, adding momentum 
uses the gradient history to give more weight to gradient updates that have consistently been in 
the same direction. This improves SGD tremendously. 

As a quick aside, Nesterov Momentum is a similar idea as momentum. It uses a look ahead though 
to improve the momentum update. You can find more info \href{https://cs231n.github.io/neural-networks-3/#sgd}{here}.

\subsubsection{RMSProp}
Before I explain RMSProp (Root Mean Squared Propagation), it's important to mention Adagrad (Adaptive
Gradient). Adagrad is a method that uses an adaptive learning rate to update our weights proposed by 
\href{https://jmlr.org/papers/v12/duchi11a.html}{Duchi et al.}. It does so 
by dividing our learning rate by the square root of the sum of the squared history of gradients. 

\begin{verbatim}
    cache += dx**2
    x += - learning_rate * dx / (np.sqrt(cache) + eps)
\end{verbatim}


This has the effect of reducing 
the learning rate when the gradient is large, which is good because we don't need a large learning rate
if the gradient is already large. The problem with Adagrad (for Deep Learning) is that the factor we divide 
our learning rate by it is monotonically increasing, meaning we can never increase the learning rate. 
This becomes problematic for deep learning as Adagrad too often squashes our learning rate before 
we finish learning. 

An improved variant of Adagrad is RMSProp. Our dividing factor is not monotonically increasing, 
producing a less aggressive learning rate decay. In practice this works much better. Funnily enough, 
the citation for RMSProp isn't a research paper but instead \href{http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}
{lecture 6, slide 29} of Geoff Hinton's Coursera class. 

\begin{verbatim}
    cache = decay_rate * cache + (1 - decay_rate) * dx**2
    x += - learning_rate * dx / (np.sqrt(cache) + eps)
\end{verbatim}

\subsubsection{Adam}
Adam is the default optimization used for updating gradients. The essential idea is that it
combines both momentum with the adaptive gradient ideas from Adagrad/RMSprop (Adagrad + Momentum 
= Adam). Here's the code below. 

\begin{verbatim}
    m = beta1*m + (1-beta1)*dx
    mt = m / (1-beta1**t)
    v = beta2*v + (1-beta2)*(dx**2)
    vt = v / (1-beta2**t)
    x += - learning_rate * mt / (np.sqrt(vt) + eps
\end{verbatim}

I don't currently have time to dive deep into the guts of Adam to give a good explanation.
For application purposes it's important to know that it usually includes bias correction term 
in the beginning and that it can be worth it to compare to Nesterov Momentum. 


\subsection{Dropout}





\section{References}

\begin{enumerate}
    \item \href{https://cs231n.github.io/neural-networks-3/#sgd}{CS 231N: NN Training II}
\end{enumerate}

\end{document}