\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\begin{document}
\noindent Author: Benjamin Smidt

\noindent Created: September 7, 2022

\noindent Last Updated: September 8, 2022
\begin{center}
\section*{Assignment 2: Softmax Classifier}
\end{center}

\subsection*{Loss}
\subsubsection{Mathematics}

\paragraph{}
The equation for the loss function of a single example of 
Multinomial Logistic Regression is:  

\begin{equation}
    L_{i} = -log(\frac{e^{f_{y_{i}}}}{\sum_{j=1}^C e^{f_j}}) 
    = -f_{y_{i}} + log(\sum_{j} e^{f_{j}})
\end{equation}

\begin{equation}
    f_{y_{i}} = f(x_{i}; W) = Wx_{i}
\end{equation}

\noindent Thus, to find the loss for the training data, we simply need 
to average the loss $L_{i}$ for each example. For SVM's, the loss was 
called \emph{hinge loss}. The loss for a Softmax Classifier is known as 
\emph{cross-entropy loss}. 

\subsubsection{Code}
\paragraph{} 
In our implementation of finding the loss, we first matrix 
multiply $XW$ and raise every value in the resulting matrix to $e$
giving us matrix $e^{f}$ which has shape NxC since $X$ and $W$ 
have shapes NxD and DxC.
(N = examples, D = feature dimensions, C = classes). ADD VECTORIZES IMPLEMENTATION HERE.

To check our implementation, we do a quick sanity check. 
Since we initialized all our weights $w_{i}$ to be very close to 0, we
expect $e^{f_{y_{i}}}$ to be one-tenth of $\sum_{j} e^{f_{j}}$ since 
$e^{0} = 1$. Since $-log(1/10) = log(10)$, $log(10)$ is approximately our 
expected loss value. 

\subsection{Gradient}

\subsubsection{Mathematics}
\paragraph{}
To find the gradient with respect to our weight 
matrix W, let's again focus on one example. We can rewrite our loss function as:

\begin{equation}
    L_{i} = -log(\frac{e^{W_{y_{i}}x_{i}}}{\sum_{j} e^{W_{j}x_{i}}}) 
\end{equation}

where $W_{y_{i}}$ represents the weights for the correct label for example $i$ and $W_{j}$ 
is the weights for any given class (including $W_{y_{i}}$) for example $i$. 
Note that since the shape 
of $W$ is DxC, each $W_{j}$ is a column of $W$. 

Let's start by reformulating our loss function a bit.

\begin{equation}
    L_{i} = -log(\frac{e^{W_{y_{i}}x_{i}}}{\sum_{j=1}^C e^{W_{j}x_{i}}}) 
\end{equation}
\begin{equation}
    L_{i} = log(\frac{\sum_{j=1}^C e^{W_{j}x_{i}}}{e^{W_{y_{i}}x_{i}}}) 
\end{equation}
\begin{equation}
    L_{i} = log({\sum_{j=1}^C e^{W_{j}x_{i}}}) - W_{y_{i}}x_{i}
\end{equation}

\noindent Then we find the gradient with respect to $W_{y_{i}}$
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{y_{i}}} = \frac{\partial}{\partial W_{y_{i}}} 
    log({\sum_{j=1}^C e^{W_{j}x_{i}}}) 
    - \frac{\partial}{\partial W_{y_{i}}}W_{y_{i}}x_{i}
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{y_{i}}} = \frac{\partial}{\partial W_{y_{i}}} 
    log({\sum_{j=1}^C e^{W_{j}x_{i}}}) 
    - x_{i}
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{y_{i}}} = \frac{1}{\sum_{j=1}^C e^{W_{j}x_{i}}}
    \frac{\partial}{\partial W_{y_{i}}} 
    (e^{W_{1}x_{i}}+...+e^{W_{y_{i}}x_{i}}+...+e^{W_{C}x_{i}})
    - x_{i}
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{y_{i}}} = \frac{x_{i}e^{W_{y_{i}}x_{i}}}
    {\sum_{j=1}^C e^{W_{j}x_{i}}} - x_{i}
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{y_{i}}} = x_{i} (\frac{e^{W_{y_{i}}x_{i}}}
    {\sum_{j=1}^C e^{W_{j}x_{i}}} - 1)
\end{equation}

\noindent The math works out similarly for the gradient with respect to $W_{j}$
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{j}} = \frac{\partial}{\partial W_{j}} 
    log({\sum_{j=1}^C e^{W_{j}x_{i}}}) 
    - W_{y_{i}}x_{i}
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{j}} = \frac{\partial}{\partial W_{j}} 
    log({\sum_{j=1}^C e^{W_{j}x_{i}}}) 
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{j}} = \frac{1}{\sum_{j=1}^C e^{W_{j}x_{i}}}
    \frac{\partial}{\partial W_{j}} 
    (e^{W_{1}x_{i}}+...+e^{W_{j}x_{i}}+...+e^{W_{C}x_{i}})
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{j}} = \frac{x_{i}e^{W_{j}x_{i}}}
    {\sum_{j=1}^C e^{W_{j}x_{i}}}
\end{equation}

\subsubsection{Code}
As before, we implement the loss and the gradient in the same function to save computation.

\section{References}
\

\end{document}